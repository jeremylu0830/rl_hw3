# 強化學習作業三：POMDP with Deep Recurrent Q-Network (DRQN) and categorical distributional RL(C51)

> **課程：** [CS885 強化學習](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-winter22/assignments.html)  
> **主題：** 部分可觀察環境下的深度強化學習

---

## 🎯 實驗目標

在 **部分可觀察的 CartPole 環境（POMDP）** 中比較 **DQN** 與 **DRQN** 的性能差異，驗證 **LSTM 記憶機制** 對處理部分可觀察性問題的關鍵作用。

### 環境特性
* **完全可觀察版本**：Agent 可觀察 4 維狀態 `[位置, 速度, 角度, 角速度]`
* **部分可觀察版本（本實驗）**：Agent **僅能觀察** 2 維狀態 `[位置, 角度]`，**無法直接獲得速度資訊**
* **挑戰**：沒有速度資訊，相同的觀察可能來自完全不同的狀態（感知混淆）

---

## 📊 實驗結果

### DQN 性能（部分可觀察環境）
<img width="640" height="480" alt="dqn" src="https://github.com/user-attachments/assets/9bc0627f-114d-4a9b-85ff-77b6db59b441" />


### DRQN 性能（部分可觀察環境）
<img width="1715" height="1361" alt="drqn_performance" src="https://github.com/user-attachments/assets/918f950f-a218-4472-a208-5e045dc47fa5" />


---

## 📈 性能對比分析

### 量化結果

| 指標 | DQN | DRQN | 改進幅度 |
|------|-----|------|----------|
| **峰值性能** | ~45 | ~195 | **+333%** |
| **穩定期性能** | ~40 | ~120 | **+200%** |
| **收斂速度** | 快（~100 episodes） | 中等（~600 episodes） | 較慢但性能更優 |
| **性能上限** | 低（~45） | 高（~195） | **4.3 倍** |
| **學習穩定性** | 穩定但受限 | 波動較大 | 高性能的代價 |

### 關鍵觀察

#### DQN 表現特徵
* ✅ **快速收斂**：約 100 episodes 即達到穩定
* ❌ **性能受限**：平均獎勵停滯在 **38-45** 之間
* ❌ **無法突破**：受限於部分可觀察性，無法超越 45 的上限
* ✅ **相對穩定**：變異性適中，但性能低

#### DRQN 表現特徵
* ⚠️ **學習較慢**：需要約 600 episodes 才達到峰值
* ✅ **性能優異**：峰值達到 **~195**（接近理論最大值 200）
* ⚠️ **後期衰退**：從峰值降至 **~120**，但仍遠優於 DQN
* ⚠️ **變異較大**：標準差範圍較 DQN 大

---

## 🧠 LSTM 層的影響分析

### 1. 時間記憶能力

#### 無記憶（DQN）的困境
```
時間 t：觀察 o_t = [位置, 角度]
        ↓
        Q 網絡獨立處理 o_t
        ↓
        Q(o_t, a) → 選擇動作
        
問題：沒有過去觀察的記憶
```

#### 有記憶（DRQN）的優勢
```
時間 t：觀察 o_t = [位置, 角度]
        ↓
        LSTM 處理：(o_t, h_{t-1}, c_{t-1})
        ↓
        隱藏狀態：(h_t, c_t) 編碼歷史
        ↓
        Q(o_t, h_t, a) → 選擇動作
        
優勢：h_t 包含從 o_0, o_1, ..., o_t 的所有資訊
```

### 2. 從觀察序列推斷隱藏狀態

在部分可觀察的 CartPole 中，Agent 只能看到 **位置** 和 **角度**，但看不到 **速度**。LSTM 能夠進行關鍵推斷：

#### 速度推斷範例
```
觀察序列：
  t=0: [x_0 = 0.00, θ_0 = 0.05]
  t=1: [x_1 = 0.02, θ_1 = 0.03]
  t=2: [x_2 = 0.05, θ_2 = 0.00]

LSTM 推斷：
  小車速度 ≈ (x_1 - x_0) / Δt = 0.02 / 0.02 = 1.0 m/s
  角速度 ≈ (θ_1 - θ_0) / Δt = -0.02 / 0.02 = -1.0 rad/s
  
動作決策：
  速度向右 + 角度減小 → 施加向左的力
```

#### DQN 的根本限制
```
單一觀察 t=1：[x_1 = 0.02, θ_1 = 0.03]

DQN 無法回答的問題：
  ❓ 小車正在向左移動還是向右移動？
  ❓ 角度正在增加還是減少？
  ❓ 系統具有什麼樣的動量？

結果：隨機或次優的動作選擇
```

### 3. 解決感知混淆（Perceptual Aliasing）

**感知混淆** 是 POMDP 的核心挑戰：不同的真實狀態產生相同的觀察。

#### 感知混淆範例
```
狀態 A：位置 = 0, 角度 = 0, 速度 = +1.0, 角速度 = +0.5
        觀察：[0, 0]
        最優動作：向左推（抵消向右運動）

狀態 B：位置 = 0, 角度 = 0, 速度 = -1.0, 角速度 = -0.5
        觀察：[0, 0]
        最優動作：向右推（抵消向左運動）
```

#### DQN 的困境
* 兩個狀態產生相同的觀察 `[0, 0]`
* `Q([0, 0], 左)` 對狀態 A 應該高，但對狀態 B 應該低
* `Q([0, 0], 右)` 對狀態 A 應該低，但對狀態 B 應該高
* **訓練信號相互衝突** → 收斂困難 → 性能受限

#### DRQN 的解決方案
* LSTM 隱藏狀態 `h_A` 編碼「向右移動」的歷史
* LSTM 隱藏狀態 `h_B` 編碼「向左移動」的歷史
* 即使觀察相同，`h_A ≠ h_B`
* `Q([0, 0], h_A, 左)` 和 `Q([0, 0], h_B, 右)` 可以同時正確學習
* **無訓練信號衝突** → 收斂更好 → 性能優異

---

## 🔬 理論解釋：恢復馬可夫性質

### 馬可夫性質的挑戰

#### 完全可觀察 MDP
* 狀態 `s_t` 包含所有必要資訊
* 馬可夫性質：`P(s_{t+1} | s_t, a_t)` 獨立於歷史
* 最優策略：`π*(a | s)`
* DQN 可以有效學習 `Q*(s, a)`

#### 部分可觀察 POMDP（本實驗）
* 僅有觀察 `o_t` 可用，無完整狀態 `s_t`
* **馬可夫性質被破壞**：`P(o_{t+1} | o_t, a_t)` 依賴於隱藏歷史
* 最優策略需要歷史：`π*(a | o_0, o_1, ..., o_t)`
* 僅基於 `o_t` 的 DQN **無法學習最優策略**

### 信念狀態構造

DRQN 有效構造了 **信念狀態**，近似真實的潛在狀態：

```
時間 t 的信念狀態：
  b_t = LSTM(o_t, h_{t-1})

其中：
  - o_t：當前觀察
  - h_{t-1}：編碼過去觀察的隱藏狀態
  - b_t = (o_t, h_t)：增強的狀態表示

關鍵性質：
  b_t 近似滿足馬可夫性質
  P(b_{t+1} | b_t, a_t) ≈ P(s_{t+1} | s_t, a_t)
```

### 數學證明

在 POMDP 中，信念狀態上的最優 Q 函數為：
```
Q*(b_t, a_t) = E[R_t + γ max_{a'} Q*(b_{t+1}, a')]
```

LSTM 隱藏狀態 `h_t` 作為歷史的充分統計量，使 DRQN 能夠近似：
```
Q_DRQN(o_t, h_t, a_t) ≈ Q*(b_t, a_t) ≈ Q*_MDP(s_t, a_t)
```

而沒有記憶的 DQN 只能學習：
```
Q_DQN(o_t, a_t) ≠ Q*(b_t, a_t)
```

這個根本差異解釋了為什麼 DRQN 能達到近乎最優的性能（~195，接近理論最大值 200），而 DQN 卻停滯在次優水平（~45）。

---

## 💡 性能差異的原因

### 資訊內容對比

| 面向 | DQN | DRQN |
|------|-----|------|
| **輸入資訊** | 單一觀察 `o_t` | 觀察 + 隱藏狀態 `(o_t, h_t)` |
| **時間範圍** | 僅當前時間步 | 透過 `h_t` 的整個歷史 |
| **狀態估計** | 不可能 | 透過序列分析可行 |
| **決策品質** | 因不確定性而次優 | 透過推斷狀態接近最優 |

**資訊理論視角**：
* DQN 接收 ~2 維資訊（位置、角度）
* DRQN 接收 ~2 + 512 維資訊（觀察 + LSTM 隱藏狀態）
* 512 維隱藏狀態編碼時間模式，恢復缺失的速度資訊

### 信用分配改進

#### DQN 的信用分配問題
```
具有感知混淆的軌跡：
  t=10: o=[0,0], a=左  → 最終獲得獎勵
  t=50: o=[0,0], a=右  → 最終獲得獎勵

TD 誤差信號：
  在 t=10：增加 Q([0,0], 左)
  在 t=50：增加 Q([0,0], 右)
  
結果：兩個 Q 值都增加 → 沒有清晰策略 → 隨機行為
```

#### DRQN 的改進信用分配
```
具有隱藏狀態的軌跡：
  t=10: o=[0,0], h=h₁₀, a=左  → 獎勵
  t=50: o=[0,0], h=h₅₀, a=右  → 獎勵

TD 誤差信號：
  在 t=10：增加 Q([0,0], h₁₀, 左)
  在 t=50：增加 Q([0,0], h₅₀, 右)
  
結果：不同的狀態表示 → 清晰的策略學習
```

此外，DRQN 的 **時間反向傳播（BPTT）** 允許梯度向後流動通過序列，使網絡能夠學習：
* 哪些過去的觀察與當前獎勵相關
* 如何在隱藏狀態中編碼時間模式
* 動作與結果之間的長期依賴關係

### 探索效率

#### DQN 的探索挑戰
* 在混淆的觀察空間中隨機探索
* 無法區分動作成功是因為運氣還是正確的狀態推斷
* 需要指數級更多樣本才能偶然發現正確的狀態-動作關聯

#### DRQN 的探索優勢
* LSTM 自動學習編碼相關歷史
* 隨著網絡改進，自然構建更好的狀態表示
* 隨著信念狀態改進，探索變得更有指導性
* 需要更少樣本來發現有效策略

---

## 📉 DRQN 性能衰退分析

實驗結果中的有趣現象：DRQN 性能從 ~195（episodes 600-800）降至 ~120（episodes 1500-2000）。

### 可能原因

#### 1. 災難性遺忘（Catastrophic Forgetting）
```
早期訓練：Buffer 包含多樣化 episodes（好的和壞的）
         → 網絡學習穩健策略

後期訓練：Buffer 被「相當好」的 episodes 主導
         → 網絡過擬合當前策略
         → 忘記如何從罕見狀態恢復
```

#### 2. 探索-利用權衡
```
ε-貪婪衰減：1.0 → 0.1（超過 10,000 步）

早期：高探索 → 發現峰值性能策略
後期：低探索 → 困在略差於峰值的局部最優
```

#### 3. 移動目標問題
```
目標網絡每 10 episodes 更新一次

隨著 LSTM，Q 值估計變得越來越複雜
→ 目標網絡落後於策略網絡
→ 從過時目標的自舉導致不穩定
```

#### 4. Replay Buffer 組成
```
Buffer 大小：10,000 episodes

峰值性能 episodes（600-800）被覆蓋
→ 網絡在更近期、可能次優的 episodes 上訓練
→ 性能向當前 buffer 分布轉移
```

### 為何 DQN 沒有衰退？

DQN 保持穩定是因為：
1. **低性能上限**：已經在部分可觀察性約束下的局部最優
2. **簡單狀態空間**：2D 觀察更容易維持穩定的 Q 估計
3. **無記憶需要遺忘**：沒有需要維持的複雜時間模式

**DQN 的穩定性不是優勢**——它只是困在一個糟糕的解決方案中。

---

## 🎓 實驗結論

### 主要發現

1. **DRQN 在 POMDP 中顯著優於 DQN**
   * 峰值性能提升 **333%**
   * 穩定期性能提升 **200%**
   * 即使後期衰退，仍保持 **3 倍** 於 DQN 的性能

2. **LSTM 層的關鍵作用**
   * ✅ 提供時間記憶能力
   * ✅ 從觀察序列推斷隱藏狀態變量（速度）
   * ✅ 消除感知混淆
   * ✅ 恢復部分馬可夫性質

3. **權衡取捨**
   * DRQN：高性能但收斂慢、穩定性稍差
   * DQN：快速收斂但性能嚴重受限

4. **核心洞察**
   * 在部分可觀察環境中，記憶機制是**必需的**，而非可選
   * 無記憶的 Agent 存在根本性缺陷，**無論訓練多久都無法補償**
   * 架構選擇（DRQN vs DQN）比超參數調整更重要

### 理論驗證

這個實驗證實了：
* **記憶對部分可觀察性的關鍵性**：DQN 僅達到理論最大值的 23%（45/200），DRQN 達到 98%（195/200）
* **序列建模可以恢復馬可夫性**：LSTM 構造的信念狀態近似最優
* **Recurrent RL 在實際應用中的優勢**：性能提升（333%）遠超過計算成本

### 實用建議

在部分可觀察環境中：
* ✅ 優先使用 DRQN/LSTM-based 方法
* ✅ 考慮使用更大的隱藏狀態（HIDDEN=512 或更高）
* ✅ 注意長期穩定性問題（可考慮 PER、更大 buffer）
* ✅ 可結合其他技術（Soft Update、較低學習率）

### 更廣泛的意義

這項工作展示了：
* **架構選擇至關重要**：在 POMDP 中，正確的架構（DRQN）比錯誤架構（DQN）的大量超參數調整更重要
* **記憶機制不是可選的**：對於具有部分可觀察性的真實世界應用，基於記憶的方法是必需的
* **原則性適用範圍**：「在部分可觀察環境中，沒有記憶的 Agent 從根本上處於劣勢」這一原則延伸到所有基於不完整資訊做決策的領域

---

