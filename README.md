# 強化學習作業三：POMDP with Deep Recurrent Q-Network (DRQN) and categorical distributional RL(C51)

> **課程：** [CS885 強化學習](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-winter22/assignments.html)  
> **主題：** 部分可觀察環境下的深度強化學習

---


## 🧪 實驗一｜DRQN vs. DQN（部分可觀察 CartPole）

在 **部分可觀察的 CartPole 環境（POMDP）** 中比較 **DQN** 與 **DRQN** 的性能差異，驗證 **LSTM 記憶機制** 對處理部分可觀察性問題的關鍵作用。

### 環境特性
* **完全可觀察版本**：Agent 可觀察 4 維狀態 `[位置, 速度, 角度, 角速度]`
* **部分可觀察版本（本實驗）**：Agent **僅能觀察** 2 維狀態 `[位置, 角度]`，**無法直接獲得速度資訊**
* **挑戰**：沒有速度資訊，相同的觀察可能來自完全不同的狀態（感知混淆）

---

## 📊 實驗結果

### DQN 性能（部分可觀察環境）
<img width="640" height="480" alt="dqn" src="https://github.com/user-attachments/assets/9bc0627f-114d-4a9b-85ff-77b6db59b441" />


### DRQN 性能（部分可觀察環境）
<img width="1715" height="1361" alt="drqn_performance" src="https://github.com/user-attachments/assets/918f950f-a218-4472-a208-5e045dc47fa5" />


---

## 📈 性能對比分析

### 量化結果

| 指標 | DQN | DRQN | 改進幅度 |
|------|-----|------|----------|
| **峰值性能** | ~45 | ~195 | **+333%** |
| **穩定期性能** | ~40 | ~120 | **+200%** |
| **收斂速度** | 快（~100 episodes） | 中等（~600 episodes） | 較慢但性能更優 |
| **性能上限** | 低（~45） | 高（~195） | **4.3 倍** |
| **學習穩定性** | 穩定但受限 | 波動較大 | 高性能的代價 |

### 關鍵觀察

#### DQN 表現特徵
* ✅ **快速收斂**：約 100 episodes 即達到穩定
* ❌ **性能受限**：平均獎勵停滯在 **38-45** 之間
* ❌ **無法突破**：受限於部分可觀察性，無法超越 45 的上限
* ✅ **相對穩定**：變異性適中，但性能低

#### DRQN 表現特徵
* ⚠️ **學習較慢**：需要約 600 episodes 才達到峰值
* ✅ **性能優異**：峰值達到 **~195**（接近理論最大值 200）
* ⚠️ **後期衰退**：從峰值降至 **~120**，但仍遠優於 DQN
* ⚠️ **變異較大**：標準差範圍較 DQN 大

---

## 🧠 LSTM 層的影響分析

### 1. 時間記憶能力

#### 無記憶（DQN）的困境
```
時間 t：觀察 o_t = [位置, 角度]
        ↓
        Q 網絡獨立處理 o_t
        ↓
        Q(o_t, a) → 選擇動作
        
問題：沒有過去觀察的記憶
```

#### 有記憶（DRQN）的優勢
```
時間 t：觀察 o_t = [位置, 角度]
        ↓
        LSTM 處理：(o_t, h_{t-1}, c_{t-1})
        ↓
        隱藏狀態：(h_t, c_t) 編碼歷史
        ↓
        Q(o_t, h_t, a) → 選擇動作
        
優勢：h_t 包含從 o_0, o_1, ..., o_t 的所有資訊
```

### 2. 從觀察序列推斷隱藏狀態

在部分可觀察的 CartPole 中，Agent 只能看到 **位置** 和 **角度**，但看不到 **速度**。LSTM 能夠進行關鍵推斷：

#### 速度推斷範例
```
觀察序列：
  t=0: [x_0 = 0.00, θ_0 = 0.05]
  t=1: [x_1 = 0.02, θ_1 = 0.03]
  t=2: [x_2 = 0.05, θ_2 = 0.00]

LSTM 推斷：
  小車速度 ≈ (x_1 - x_0) / Δt = 0.02 / 0.02 = 1.0 m/s
  角速度 ≈ (θ_1 - θ_0) / Δt = -0.02 / 0.02 = -1.0 rad/s
  
動作決策：
  速度向右 + 角度減小 → 施加向左的力
```

#### DQN 的根本限制
```
單一觀察 t=1：[x_1 = 0.02, θ_1 = 0.03]

DQN 無法回答的問題：
  ❓ 小車正在向左移動還是向右移動？
  ❓ 角度正在增加還是減少？
  ❓ 系統具有什麼樣的動量？

結果：隨機或次優的動作選擇
```

### 3. 解決感知混淆（Perceptual Aliasing）

**感知混淆** 是 POMDP 的核心挑戰：不同的真實狀態產生相同的觀察。

#### 感知混淆範例
```
狀態 A：位置 = 0, 角度 = 0, 速度 = +1.0, 角速度 = +0.5
        觀察：[0, 0]
        最優動作：向左推（抵消向右運動）

狀態 B：位置 = 0, 角度 = 0, 速度 = -1.0, 角速度 = -0.5
        觀察：[0, 0]
        最優動作：向右推（抵消向左運動）
```

#### DQN 的困境
* 兩個狀態產生相同的觀察 `[0, 0]`
* `Q([0, 0], 左)` 對狀態 A 應該高，但對狀態 B 應該低
* `Q([0, 0], 右)` 對狀態 A 應該低，但對狀態 B 應該高
* **訓練信號相互衝突** → 收斂困難 → 性能受限

#### DRQN 的解決方案
* LSTM 隱藏狀態 `h_A` 編碼「向右移動」的歷史
* LSTM 隱藏狀態 `h_B` 編碼「向左移動」的歷史
* 即使觀察相同，`h_A ≠ h_B`
* `Q([0, 0], h_A, 左)` 和 `Q([0, 0], h_B, 右)` 可以同時正確學習
* **無訓練信號衝突** → 收斂更好 → 性能優異

---

## 💡 性能差異的原因

### 資訊內容對比

| 面向 | DQN | DRQN |
|------|-----|------|
| **輸入資訊** | 單一觀察 `o_t` | 觀察 + 隱藏狀態 `(o_t, h_t)` |
| **時間範圍** | 僅當前時間步 | 透過 `h_t` 的整個歷史 |
| **狀態估計** | 不可能 | 透過序列分析可行 |
| **決策品質** | 因不確定性而次優 | 透過推斷狀態接近最優 |

**資訊理論視角**：
* DQN 接收 ~2 維資訊（位置、角度）
* DRQN 接收 ~2 + 512 維資訊（觀察 + LSTM 隱藏狀態）
* 512 維隱藏狀態編碼時間模式，恢復缺失的速度資訊

### 信用分配改進

#### DQN 的信用分配問題
```
具有感知混淆的軌跡：
  t=10: o=[0,0], a=左  → 最終獲得獎勵
  t=50: o=[0,0], a=右  → 最終獲得獎勵

TD 誤差信號：
  在 t=10：增加 Q([0,0], 左)
  在 t=50：增加 Q([0,0], 右)
  
結果：兩個 Q 值都增加 → 沒有清晰策略 → 隨機行為
```

#### DRQN 的改進信用分配
```
具有隱藏狀態的軌跡：
  t=10: o=[0,0], h=h₁₀, a=左  → 獎勵
  t=50: o=[0,0], h=h₅₀, a=右  → 獎勵

TD 誤差信號：
  在 t=10：增加 Q([0,0], h₁₀, 左)
  在 t=50：增加 Q([0,0], h₅₀, 右)
  
結果：不同的狀態表示 → 清晰的策略學習
```

此外，DRQN 的 **時間反向傳播（BPTT）** 允許梯度向後流動通過序列，使網絡能夠學習：
* 哪些過去的觀察與當前獎勵相關
* 如何在隱藏狀態中編碼時間模式
* 動作與結果之間的長期依賴關係

### 探索效率

#### DQN 的探索挑戰
* 在混淆的觀察空間中隨機探索
* 無法區分動作成功是因為運氣還是正確的狀態推斷
* 需要指數級更多樣本才能偶然發現正確的狀態-動作關聯

#### DRQN 的探索優勢
* LSTM 自動學習編碼相關歷史
* 隨著網絡改進，自然構建更好的狀態表示
* 隨著信念狀態改進，探索變得更有指導性
* 需要更少樣本來發現有效策略

---

## 📉 DRQN 性能衰退分析

實驗結果中的有趣現象：DRQN 性能從 ~195（episodes 600-800）降至 ~120（episodes 1500-2000）。

### 可能原因

#### 1. 災難性遺忘（Catastrophic Forgetting）
```
早期訓練：Buffer 包含多樣化 episodes（好的和壞的）
         → 網絡學習穩健策略

後期訓練：Buffer 被「相當好」的 episodes 主導
         → 網絡過擬合當前策略
         → 忘記如何從罕見狀態恢復
```

#### 2. 探索-利用權衡
```
ε-貪婪衰減：1.0 → 0.1（超過 10,000 步）

早期：高探索 → 發現峰值性能策略
後期：低探索 → 困在略差於峰值的局部最優
```

#### 3. 移動目標問題
```
目標網絡每 10 episodes 更新一次

隨著 LSTM，Q 值估計變得越來越複雜
→ 目標網絡落後於策略網絡
→ 從過時目標的自舉導致不穩定
```

#### 4. Replay Buffer 組成
```
Buffer 大小：10,000 episodes

峰值性能 episodes（600-800）被覆蓋
→ 網絡在更近期、可能次優的 episodes 上訓練
→ 性能向當前 buffer 分布轉移
```

### 為何 DQN 沒有衰退？

DQN 保持穩定是因為：
1. **低性能上限**：已經在部分可觀察性約束下的局部最優
2. **簡單狀態空間**：2D 觀察更容易維持穩定的 Q 估計
3. **無記憶需要遺忘**：沒有需要維持的複雜時間模式

**DQN 的穩定性不是優勢**——它只是困在一個糟糕的解決方案中。

---

## 🎓 實驗結論

### 主要發現

1. **DRQN 在 POMDP 中顯著優於 DQN**
   * 峰值性能提升 **333%**
   * 穩定期性能提升 **200%**
   * 即使後期衰退，仍保持 **3 倍** 於 DQN 的性能

2. **LSTM 層的關鍵作用**
   * ✅ 提供時間記憶能力
   * ✅ 從觀察序列推斷隱藏狀態變量（速度）
   * ✅ 消除感知混淆
   * ✅ 恢復部分馬可夫性質

3. **權衡取捨**
   * DRQN：高性能但收斂慢、穩定性稍差
   * DQN：快速收斂但性能嚴重受限

4. **核心洞察**
   * 在部分可觀察環境中，記憶機制是**必需的**，而非可選
   * 無記憶的 Agent 存在根本性缺陷，**無論訓練多久都無法補償**
   * 架構選擇（DRQN vs DQN）比超參數調整更重要

## 🎰 實驗二｜C51 vs. DQN（隨機環境 CartPole）

**核心問題**：學習獎勵分佈（而非點估計）是否能改善性能？

### 性能對比

| 指標 | DQN | C51 | 改進幅度 |
|------|-----|-----|----------|
| **峰值性能** | ~500 | ~500 | **持平** |
| **穩定期平均** | ~300 | ~420 | **+40%** |
| **收斂速度** | 快（~50 eps） | 快（~100 eps） | 稍慢 |
| **性能穩定性** | σ ≈ 100+ | σ ≈ 50 | **-50% 穩定** |
| **最低性能保證** | ~150 | ~300 | **確保下限** |

### 實驗觀察
<img width="640" height="480" alt="c51_modified" src="https://github.com/user-attachments/assets/f95402fe-c120-4ad2-833f-0869bda46a6b" />

#### C51 的表現特徵
- ✅ **快速收斂**：與 DQN 相近（~50-100 episodes）
- ✅ **穩定性優異**：標準差顯著減小（50% 更穩定）
- ✅ **性能均衡**：維持在 400-500 的較高水平


<img width="640" height="480" alt="dqn_results" src="https://github.com/user-attachments/assets/3e0b5833-ddce-4650-a1fc-7a7c97095d32" />

#### DQN 的表現特徵
- ✅ **快速收斂**：50 episodes 達到較高水平
- ❌ **極端波動**：在 300–450 episodes 間，DQN 的某些 run 會掉到 ~150 左右，導致標準差變很大
- ❌ **風險較大**：標準差極大，無下限保障，性能難以預測


### 對比發現

#### 1. 收斂曲線分析

**DQN 曲線特徵**：
```
Phase 1 (0-50 eps)：快速上升
  從 ~20 → ~400
  
Phase 2 (50-250 eps)：緩慢上升
  從 ~400 → ~500
  
Phase 3 (250-450 eps)：劇烈波動 ⚠️
  從 ~500 → ~150 (極度波動!)
  標準差：100+
  
Phase 4 (450-500 eps)：部分恢復
  從 ~150 → ~450
  但仍極不穩定
```

**C51 曲線特徵**：
```
Phase 1 (0-100 eps)：快速上升
  從 ~20 → ~450
  
Phase 2 (100-500 eps)：穩定維持
  維持在 ~420-480 範圍
  標準差：~50（穩定！）
  
關鍵差異：
- C51 避免了 DQN 的災難性衰退
- C51 標準差恆定較小
- C51 性能更可預測
```

#### 2. 穩定性指標對比

```
DQN 的不穩定性根源：

時間段          平均獎勵    標準差    波動性評分
0-100 eps       ~250        ~80       中
100-250 eps     ~450        ~70       中
250-350 eps     ~350        ~150      極高 ⚠️
350-450 eps     ~280        ~200      極高 ⚠️
450-500 eps     ~400        ~120      高 ⚠️

結論：後期劇烈不穩定


C51 的穩定性優勢：

時間段          平均獎勵    標準差    波動性評分
0-100 eps       ~250        ~80       中
100-250 eps     ~450        ~50       低
250-350 eps     ~440        ~45       低 ✅
350-450 eps     ~430        ~50       低 ✅
450-500 eps     ~410        ~55       低 ✅

結論：全程穩定
```

#### 3. 為什麼 C51 更穩定？

**理論分析**：

```
DQN 的學習目標：
  Loss = (R + γ max Q(s') - Q(s,a))²
  
問題：
  1. 單一 max 操作容易誤估
  2. 異常高獎勵造成大梯度
  3. 無自校正機制

範例：
  正常軌跡：R = 50, γ max Q(s') = 400
  → Target = 450
  
  異常軌跡：R = 500, γ max Q(s') = 250 (誤估)
  → Target = 750 (錯誤!)
  
  DQN 被迫學習錯誤的 Q 值 → 決策變差 → 性能下降


C51 的學習目標：
  Loss = KL(P_target || P_predict)
  
優勢：
  1. 整個分佈參與損失
  2. 異常值被自然平滑
  3. 分佈形狀提供自我檢查

範例（同上軌跡）：
  正常分佈：P(R|s,a) 集中在 [400-500]
  
  異常樣本：即使 R=500，C51 看到
  - 歷史軌跡主要在 [300-450]
  - 單個 500 獎勵的概率被降權
  - 整個分佈受影響但不至於完全改變
  
  結果：穩定學習
```






- **C51 是改進**（穩定性提升）  
- **DRQN 是突破**（根本解決）
- **DRQN + C51 是完美**（性能與穩定的統一）

選擇正確的工具，勝過磨練最好的技術。

